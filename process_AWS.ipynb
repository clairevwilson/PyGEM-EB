{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data Processing\n",
    "This Jupyter notebook contains code to process automatic weather station (AWS) data to a standardized format for the PyGEM-EB model. The AWS dataset should be in a single comma/tab-separated file containing all timesteps and all data variables. If the data is not in that format, first use preprocess_AWS.ipynb. Note that this code may not be comprehensive for all errors that arise from the formatting of a specific dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import xarray as xr\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up: Define variables\n",
    "Fill out the cell below to set up the glacier being accessed and the filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLACIER INFORMATION   \n",
    "# change these\n",
    "glac_name = 'naluday'        # Glacier name for output file         \n",
    "station_name = '2024'               # You choose: descriptive name for log and output (e.g. site name, year, etc.)\n",
    "elev = 1419                     # Elevation of AWS [m a.s.l.]\n",
    "lat = 60.30271                # Latitude of AWS [decimal degrees]\n",
    "lon = -138.57565              # Longitude of AWS [decimal degrees]\n",
    "station_type = 'on-ice'         # Type of station (on-ice, off-ice, or debris)\n",
    "\n",
    "# FILEPATHS\n",
    "# change these\n",
    "data_fn = 'Raw/naluday/preprocessed_2024.csv'               # Filename of input data (relative to this notebook)\n",
    "# don't change these\n",
    "data_fp = '../climate_data/AWS/'                            # Filepath to the folder of input data\n",
    "export_fp = '../climate_data/AWS/Processed/'                # Filepath to the folder to save this data\n",
    "export_fn = export_fp + glac_name +'/'\n",
    "if not os.path.exists(export_fn):\n",
    "    os.mkdir(export_fn)                                     # Make folder for this glacier\n",
    "export_fn += glac_name + station_name + '.csv'              # Name of output file\n",
    "metadata_fn = 'data/aws_metadata.txt'                       # Name of metadata file\n",
    "if not os.path.exists(metadata_fn):   \n",
    "    print('Creating metadata file for weather stations')             \n",
    "    with open(metadata_fn, 'w') as f:\n",
    "        f.write('glacier\\tstation\\televation\\tlatitude\\tlongitude\\ttype\\n')         # Write metadata text file header\n",
    "\n",
    "# TIMEZONE\n",
    "# change these\n",
    "timezone = 'GMT-07'             # Timezone of output data (should be local time)\n",
    "input_timezone = 'GMT-00'       # Time zone of input data\n",
    "\n",
    "# MINIMUM DATA PERCENTAGE\n",
    "# don't change these\n",
    "data_min_percentage = 0.8       # Below this threshold of data, AWS data will be thrown out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "From the output of the first cell, make sure the columns have been properly loaded. \n",
    "\n",
    "The cell below contains a dictionary with the possible for each variable. If the column names you want to use are missing, add those to the \"names\" dictionary.\n",
    "\n",
    "Also specify the name of the time column and any NaN values specific to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and make sure the columns were correctly loaded\n",
    "# NOTE this step is likely to cause errors that can be fixed by adjusting the input arguments, for example delim_whitespace\n",
    "rows_to_skip = 0     # Number of rows that contain text at the beginning of the file (34 for Storglaciaren), sep='\\t'\n",
    "df = pd.read_csv(data_fp + data_fn,skiprows=rows_to_skip,sep=',') \n",
    "print(df.columns.to_numpy())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out after executing above to check the variable names for time and precipitation\n",
    "time_vn = 'Unnamed: 0' \n",
    "assert time_vn in df.columns, 'Fill out time variable name'\n",
    "\n",
    "names = {'temp':['site_temp_USGS','temperature','Tair_aws','temp','TA_2.0m','T','AirTC_Avg','Temperature'],\n",
    "            'tp':['Precip_Weighing_Incremental','precipitation','Ptotal_aws','tp','P'],\n",
    "            'rh':['RelHum','RH','rh','rH','RH_aws','RH_2.0m'],\n",
    "            'SWin':['RadiationIn','SWin','SWin_aws','SW_IN','SWUpper_Avg'],\n",
    "            'SWout':['RadiationOut','SWout','SWout_aws','SW_out','SW_OUT','SWLower_Avg'],\n",
    "            'LWin':['LWRadiationIn','LWin','LWin_aws','LW_in','LW_IN','Lwin'],\n",
    "            'LWout':['LWRadiationOut','LWout','LWout_aws','LW_OUT'],\n",
    "            'NetRad':['NetRad'],\n",
    "            'wind':['WindSpeed','wind','Wind','ws_aws','WS','WS_ms_Avg'],\n",
    "            'winddir':['VecAvgWindDir','WindDir','Winddir','winddir','WD'],\n",
    "            'sp':['barom','sp','press','Press_aws','Barom','BV_BP_Avg','Pressure'],\n",
    "            'tcc':['cloud_fraction','tcc','CCF','CCF_aws'],\n",
    "            'dtemp':['dtemp','dewpoint_temp']}\n",
    "\n",
    "nan_values = [-888.8800,-888.9] # ,107.572,178.215,178.916]\n",
    "for nan_value in nan_values:\n",
    "    df = df.where(df != nan_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Space for extra preprocessing if opening the file isn't enough: =====\n",
    "# n = len(df['TIMESTAMP'])\n",
    "# store = []\n",
    "# for i in range(n):\n",
    "#     datetime = str(df.index[i]) +' '+ df['TIMESTAMP'][i]\n",
    "#     store.append(datetime)\n",
    "# df['TIMESTAMP'] = store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is sub-hourly, resampling requires the variable name for precipitation to be explicitly stated in 'precip_vn' so it can be summed rather than averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== If precipitation data is cumulative =====\n",
    "precip_vn = 'Precip'     # This line can be skipped if the data is already hourly or if there is no precip\n",
    "# orig = df[precip_vn].to_numpy().copy()\n",
    "# orig[1:] = np.diff(orig)\n",
    "# print(orig)\n",
    "# df[precip_vn] = orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Launch logger\n",
    "The logger stores information regarding the process and is saved alongside the final output .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base logger\n",
    "base_logger = logging.getLogger()\n",
    "\n",
    "# Create logger which is only updated if a line has not already been documented\n",
    "class UniqueLogger:\n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.logged_messages = set()\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    def info(self, msg):\n",
    "        if msg not in self.logged_messages:\n",
    "            self.logger.info(msg)\n",
    "            self.logged_messages.add(msg)\n",
    "\n",
    "# Check if log filepath already exists; if so, overwrite\n",
    "export_logger_fn = export_fn.replace('csv','log')\n",
    "if os.path.exists(export_logger_fn):\n",
    "    os.remove(export_logger_fn)\n",
    "\n",
    "# Add log filepath to logger and get unique logger\n",
    "fhandler = logging.FileHandler(filename=export_logger_fn)\n",
    "base_logger.addHandler(fhandler)\n",
    "logger = UniqueLogger(base_logger)\n",
    "\n",
    "# Initiate logger\n",
    "today = str(pd.Timestamp.today()).replace('-','_')[0:10]\n",
    "logger.info(f'Data for {glac_name} prepared on {today}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Resample to hourly\n",
    "Add the time index to the dataframe. If the original data has a sub-hourly frequency, resample by taking averages of each variable, with the exception of precipitation which is summed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: add start and end time to clip the data\n",
    "\n",
    "force_dates = False\n",
    "# force_dates = ['start date','end date']         # Fill with datetime strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set time as the dataframe index and remove the time column\n",
    "data_start = pd.to_datetime(df[time_vn].iloc[0])\n",
    "data_end = pd.to_datetime(df[time_vn].to_numpy()[-1])\n",
    "freq = pd.to_datetime(df[time_vn].iloc[1]) - data_start\n",
    "df = df.set_index(pd.to_datetime(df[time_vn]))\n",
    "\n",
    "# Adjust timezone of index if necessary\n",
    "if timezone != input_timezone:\n",
    "    time_diff = float(timezone[-3:]) - float(input_timezone[-3:])\n",
    "    df = df.set_index(pd.to_datetime(df.index) + pd.Timedelta(hours=time_diff))\n",
    "\n",
    "# Clip data to forced dates\n",
    "if force_dates:\n",
    "    data_start = pd.to_datetime(force_dates[0])\n",
    "    data_end = pd.to_datetime(force_dates[1])\n",
    "    df = df.loc[data_start:data_end]\n",
    "\n",
    "# Log the time\n",
    "logger.info(f'Data extends from {data_start} to {data_end} with frequency {freq.seconds / 60} min')\n",
    "ntimesteps = np.shape(pd.date_range(data_start,data_end,freq='h'))[0]\n",
    "df = df.drop(time_vn,axis=1).astype(float)\n",
    "\n",
    "# Resample hourly\n",
    "df_ = df\n",
    "if freq.seconds / 3600 < 1:\n",
    "    cols_noP = np.delete(df_.columns.to_numpy(),np.where(df_.columns.to_numpy()==precip_vn))\n",
    "    df = df_[cols_noP].resample('h').mean()\n",
    "    df['tp'] = df_[precip_vn].resample('h').sum()\n",
    "    df = df.loc[data_start:data_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rename variables\n",
    "The following code should rename inconsistent naming to that used in PyGEM-EB. It will print any variable names that were passed from the original file but were not renamed. Check this list to make sure this list doesn't contain any data you need, as it will be removed in the next step.\n",
    "\n",
    "! If this list contains a data variable you need, the printed name was not included in the list of possible options. Uncomment the block of code labeled 'UPDATE NAMES', add the corresponding variable names, and rerun the renaming block. (Or manually add these names to the 'names' variable above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAMING\n",
    "drop_vars = [] # Storage for variables to be removed\n",
    "\n",
    "# List all variables\n",
    "all_vars = ['temp','tp','rh','SWin','SWout',\n",
    "            'LWin','LWout','wind','winddir',\n",
    "            'sp','tcc','NetRad','dtemp']\n",
    "\n",
    "# Loop through dataset variables and try to rename\n",
    "for var in df.columns.to_numpy():\n",
    "    renamed = False\n",
    "\n",
    "    # Loop through each actual variable and check if the column exists\n",
    "    for var_check in all_vars:\n",
    "        if var in names[var_check]:\n",
    "            # Match found made: rename variable\n",
    "            df = df.rename(columns={var:var_check})\n",
    "\n",
    "            # Remove this variable such that only one column can exist\n",
    "            all_vars.remove(var_check)\n",
    "\n",
    "            # Store that the variable was renamed\n",
    "            renamed = True\n",
    "\n",
    "    # Drop variables that were not renamed\n",
    "    if not renamed:\n",
    "        drop_vars.append(var)\n",
    "\n",
    "# Print missing variables\n",
    "if len(drop_vars) > 0:\n",
    "    print('Variables were not renamed, including:')\n",
    "    print(drop_vars)\n",
    "    print('Read this list and make sure it only includes variables you want to drop')\n",
    "else:\n",
    "    drop_vars = [0]\n",
    "\n",
    "# Drop the variables that weren't identified in \"names\"\n",
    "df = df.drop(drop_vars,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpolate data and inspect\n",
    "First throw out any data variables that don't meet the minimum data requirement. Then fill minor data holes with interpolation. Then check the data count to look for big data gaps that weren't filled by interpolation. This likely indicates the sensor was down for a period or wasn't installed until some time into the date range. That variable will be thrown out too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through data\n",
    "for var in df.columns:\n",
    "    # Check how many of the timestamps are not NaNs\n",
    "    percent_there = df[var].count() / ntimesteps\n",
    "\n",
    "    # Below minimum data percentage, delete the column\n",
    "    if percent_there < data_min_percentage:\n",
    "        missing_percent = (1-data_min_percentage) * 100\n",
    "        statement = f'Variable {var} removed: missing over {missing_percent:.0f}% of the data'\n",
    "        print(statement)\n",
    "        logger.info(statement)\n",
    "        df = df.drop(columns=var)\n",
    "    \n",
    "# Interpolate to fill remaining gaps\n",
    "df = df.interpolate('linear')\n",
    "\n",
    "# Double check the gaps were filled (long gaps can be missed)\n",
    "for var in df.columns:\n",
    "    # Only check not-full variables with more than 0 data points\n",
    "    if df[var].count() < ntimesteps and df[var].count() > 0:\n",
    "        # Check how much data is still missing\n",
    "        nmissing = ntimesteps - df[var].count()\n",
    "        missing = df[var][df[var].isna()]\n",
    "\n",
    "        if len(missing) > 0:\n",
    "            # Print a warning\n",
    "            start_missing = str(missing.index[0])[0:10]\n",
    "            end_missing = str(missing.index[-1])[0:10]\n",
    "            statement = f'Variable {var} removed: missing {nmissing} values between {start_missing} and {end_missing} after interpolation'\n",
    "            print(statement)\n",
    "            logger.info(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check units\n",
    "Check the units of each variable. The best way to do this is manually specify the units of the variables you're using from the input data. The table below contains the units each variable is supposed to be in for the model.\n",
    "\n",
    "| Variable      | Units |\n",
    "| ----------- | ----------- |\n",
    "| Temperature      | C       |\n",
    "| Humidity   | % (0-100)        |\n",
    "| Wind Speed      | m/s       |\n",
    "| Wind Direction   | $\\circ$        |\n",
    "| Precipitation      | m (w.e.)       |\n",
    "| Surface Pressure   | Pa        |\n",
    "| Incoming Shortwave      | J/m$^2$       |\n",
    "| Incoming Longwave   | J/m$^2$        |\n",
    "| Cloud cover      | 0-1 (decimal)       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== If the units are specified in a row of the original dataframe, print and inspect ======\n",
    "# df_units = pd.read_csv(data_fp + data_fn,skiprows=rows_to_skip,delim_whitespace=False) \n",
    "# print(df_units.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL DATA UNITS (FILL THIS OUT FOR THE VARIABLES IN df)\n",
    "temp_units = 'C'\n",
    "tp_units = 'm'\n",
    "sp_units = 'mbar'\n",
    "rh_units = '%'\n",
    "wind_units = 'km hr-1'\n",
    "winddir_units = 'degrees'\n",
    "SWin_units = 'W m-2'\n",
    "SWout_units = 'W m-2'\n",
    "LWin_units = 'W m-2'\n",
    "LWout_units = 'W m-2'\n",
    "tcc_units = 'decimal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out units of input data\n",
    "units_in = {'temp':temp_units,'rh':rh_units,'SWin':SWin_units,'SWout':SWout_units,'LWin':LWin_units,'LWout':LWout_units,\n",
    "            'wind':wind_units,'winddir':winddir_units,'sp':sp_units,'tp':tp_units,'tcc':tcc_units}\n",
    "\n",
    "# Units we should be in\n",
    "units_out = {'temp':'C','dtemp':'K','tp':'m','rh':'%',\n",
    "        'SWin':'J m-2','SWout':'J m-2','LWin':'J m-2','LWout':'J m-2',\n",
    "        'tcc':'0-1','wind':'m s-1','winddir':'deg','sp':'Pa'}\n",
    "\n",
    "# Check each variable for common unit differences\n",
    "if 'temp' in df.columns:\n",
    "    if units_in['temp'] == 'K':\n",
    "        df['temp'] = df['temp'] - 273.15\n",
    "        print('Adjusted temp K-->C')\n",
    "        units_in['temp'] = 'C'\n",
    "    elif units_in['temp'] == 'F':\n",
    "        df['temp'] = (df['temp'] - 32)*5/9\n",
    "        print('Adjusted temp F-->C')\n",
    "        units_in['temp'] = 'C'\n",
    "    \n",
    "if 'rh' in df.columns:\n",
    "    if units_in['rh'] == 'decimal':\n",
    "        df['rh'] = df['rh'] * 100\n",
    "        print('Adjusted RH decimal-->%')\n",
    "        units_in['rh'] = '%'\n",
    "\n",
    "if 'wind' in df.columns:\n",
    "    if units_in['wind'] == 'km hr-1':\n",
    "        df['wind'] = df['wind'] * 1000 / 3600\n",
    "        print('Adjusted wind speed-->m s-1')\n",
    "        units_in['wind'] = 'm s-1'\n",
    "    \n",
    "if 'winddir' in df.columns:\n",
    "    if units_in['winddir'] == 'rad':\n",
    "        df['winddir'] = df['winddir'] * 180/np.pi\n",
    "        print('Adjusted winddir radians-->deg')\n",
    "        units_in['winddir'] = 'deg'\n",
    "    \n",
    "if 'tcc' in df.columns:\n",
    "    if units_in['tcc'] == '%':\n",
    "        df['tcc'] = df['tcc'] / 100\n",
    "        print('Adjusted tcc %-->0-1')\n",
    "        units_in['tcc'] = '0-1'\n",
    "    \n",
    "if 'tp' in df.columns:\n",
    "    if units_in['tp'] == 'm s-1':\n",
    "        df['tp'] = df['tp'] *3600\n",
    "        print('Adjusted tp m/s-->m')\n",
    "        units_in['tp'] = 'm'\n",
    "    elif units_in['tp'] == 'mm':\n",
    "        df['tp'] = df['tp'] / 1000\n",
    "        print('Adjusted tp mm-->m')\n",
    "        units_in['tp'] = 'm'\n",
    "\n",
    "for rad in ['SWin','SWout','LWin','LWout']:\n",
    "    if rad in df.columns:\n",
    "        if units_in[rad] == 'W m-2':\n",
    "            df[rad] = df[rad] * 3600\n",
    "            print(f'Adjusted {rad} W m-2-->J m-2')\n",
    "            units_in[rad] = 'J m-2'\n",
    "\n",
    "if 'sp' in df.columns:\n",
    "    if units_in['sp'] == 'mmHg':\n",
    "        df['sp'] = df['sp'] * 133.32\n",
    "        print('Adjusted sp mmHg-->Pa')\n",
    "        units_in['sp'] = 'Pa'\n",
    "    elif units_in['sp'] == 'cmHg':\n",
    "        df['sp'] = df['sp'] * 1333.2\n",
    "        print('Adjusted sp cmHg-->Pa')\n",
    "        units_in['sp'] = 'Pa'\n",
    "    elif units_in['sp'] == 'kPa':\n",
    "        df['sp'] = df['sp'] * 1000\n",
    "        print('Adjusted sp kPa-->Pa')\n",
    "        units_in['sp'] = 'Pa'\n",
    "    elif units_in['sp'] in ['mbar','hPa']:\n",
    "        df['sp'] = df['sp'] * 100\n",
    "        units_sp = units_in['sp']\n",
    "        print(f'Adjusted sp {units_sp}-->Pa')\n",
    "        units_in['sp'] = 'Pa'\n",
    "\n",
    "# Check SW fluxes make sense (SWout cannot exceed SWin)\n",
    "if 'SWout' in df.columns:\n",
    "    df['SWout'] = df['SWout'].mask(df['SWout'] - df['SWin'] > 0,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reasonable value filters\n",
    "Pass over each data variable and remove any unreasonable values by setting them to a prescribed limit. You can choose to 'remove' (any rows with any variables outside of their range are removed) or 'clip' (clips extreme values to their bound). If you want to perform a different operation on any variables, you can add 'method' to the var_bounds dictionary under that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_bounds = 'remove' # Choose between 'remove' (remove rows with any variable outside of its range) and 'clip' (clip values to their bounds)\n",
    "var_bounds = {'temp':{'low':-60,'high':50},\n",
    "                'dtemp':{'low':-60,'high':30},\n",
    "                'tp':{'low':0,'high':0.15},\n",
    "                'rh':{'low':0,'high':100},\n",
    "                'SWin':{'low':0,'high':1400*3600,'method':'clip'},\n",
    "                'SWout':{'low':0,'high':1400*3600},\n",
    "                'LWin':{'low':0,'high':500*3600},\n",
    "                'LWout':{'low':0,'high':500*3600},\n",
    "                'tcc':{'low':0,'high':1},\n",
    "                'wind':{'low':0,'high':70},\n",
    "                'winddir':{'low':0,'high':360},\n",
    "                'sp':{'low':75000,'high':110000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in var_bounds:\n",
    "    if var in df.columns:\n",
    "        count_first = df[var].count()\n",
    "\n",
    "        # check if need a special method for this var\n",
    "        if 'method' in var_bounds[var]:\n",
    "            method = var_bounds[var]['method']\n",
    "        else:\n",
    "            method = method_bounds\n",
    "        \n",
    "        # remove or clip the data\n",
    "        if method == 'remove':\n",
    "            df[var] = df[var].where(df[var].between(var_bounds[var]['low'], var_bounds[var]['high']))\n",
    "            n_removed =  count_first - df[var].count()\n",
    "            print(f'Removed {n_removed} datapoints of {var}')\n",
    "\n",
    "            start = df[df.notna().all(axis=1)].index.min()\n",
    "            end = df[df.notna().all(axis=1)].index.max()\n",
    "            df = df.loc[start:end]\n",
    "\n",
    "        elif method == 'clip':\n",
    "            df[var] = df[var].clip(var_bounds[var]['low'], var_bounds[var]['high'])\n",
    "\n",
    "if method_bounds == 'remove':\n",
    "    # Interpolate to fill new gaps introduced\n",
    "    df = df.interpolate('linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for remaining gaps\n",
    "assert df.isna().sum().sum() == 0, 'Still have data gaps!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on wind speed and calculate directional terms if winddir is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'wind' in df.columns:\n",
    "    if 'winddir' in df.columns:\n",
    "        #     df['uwind'] = \n",
    "        #     df['vwind'] = \n",
    "        df = df.drop(columns='winddir')\n",
    "    df = df.rename(columns={'wind':'uwind'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final check and export\n",
    "Print the dataframe and plot the data and visually inspect. Look for any weird values and make sure the daily profiles seem about right (e.g., peak sunlight around noon). Finally export the data and metadata!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([df.head(3), df.tail(3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many variables there are \n",
    "full_vars = []\n",
    "for var in df.columns:\n",
    "    if df[var].count() > 0:\n",
    "        full_vars.append(var)\n",
    "logger.info(f'Data contains: '+', '.join(full_vars))\n",
    "\n",
    "# make subplots\n",
    "nvars = len(full_vars)\n",
    "fig,axis = plt.subplots(nvars,1,figsize=(7,1.5*nvars),sharex=True)\n",
    "\n",
    "# add hour of day column\n",
    "df['hour'] = pd.to_datetime(df.index).hour\n",
    "\n",
    "# loop through and plot variables\n",
    "for i,var in enumerate(full_vars):\n",
    "    var_hourly = []\n",
    "    for hour in np.arange(24):\n",
    "        # select the dataframe by the hour and find the mean\n",
    "        ds_hour = df[df['hour'] == hour]\n",
    "        hourly_mean = np.mean(ds_hour[var])\n",
    "        var_hourly.append(hourly_mean)\n",
    "    axis[i].plot(np.arange(24),var_hourly)\n",
    "    axis[i].set_ylabel(var)\n",
    "    axis[i].set_xlim((0,23))\n",
    "axis[i].set_xlabel('Hour of day')\n",
    "axis[i].set_xticks([0,6,12,18])\n",
    "plt.show()\n",
    "\n",
    "df = df.drop(columns=['hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['SWout','LWout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data\n",
    "if 'SWout' in df.columns:\n",
    "    export_fn_use = export_fn.replace('.csv','allrad.csv')\n",
    "else:\n",
    "    export_fn_use = export_fn\n",
    "df.to_csv(export_fn_use)\n",
    "print(f'Saved data to {export_fn_use}')\n",
    "\n",
    "# Store metadata\n",
    "new_line = f'{glac_name}\\t{station_name}\\t{elev}\\t{lat}\\t{lon}\\t{station_type}\\n'\n",
    "\n",
    "# Read existing lines to check if this station is already written\n",
    "with open(metadata_fn, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    existing_lines = set(line.strip() for line in lines[1:])\n",
    "\n",
    "# Write new lines\n",
    "if new_line.split('\\n')[0] not in existing_lines:\n",
    "    with open(metadata_fn, 'a') as f:\n",
    "        f.write(new_line)\n",
    "    print(f'& metadata to {metadata_fn}')\n",
    "else:\n",
    "    print('& metadata is already written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DONE! ====="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
